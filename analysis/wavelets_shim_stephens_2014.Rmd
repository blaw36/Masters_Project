---
title: "Wavelet-based Genetic Association Analysis... - Shim and Stephens (2014)"
date: "8 August 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Full title: _Wavelet-based genetic association analysis of functional phenotypes arising from high-throughput sequencing assays_

Paper can be found [here](https://projecteuclid.org/euclid.aoas/1437397106)

***

### Introduction

* High-level aim: Associate observable traits/behaviours with genetic variants - genetic association studies
* Data is obtained from high-throughput sequencing assays, resulting in high-resolution data, which is not utilised by traditional association analysis methods (coarser resolution);
    + Fixed length windows
    + Genes
* Treat sequence data as measuring an underlying 'function' which varies on the genome
    + Use the high-res, wavelet-based methods for functional data analysis
* Focus in this paper is on identifying _genetic variants associated with chromatin accessibility_, using _DNase-seq_
* Wavelet-based test for __association between a covariate__ _(say, a genotype)_ __and the shape of the underlying function__
    + Also provide ability to estimate the shape of genotype efffect
* Data is wavelet transformed
    + Model associations in the transformed space
    + Spatial structure of data translates to sparse structure in wavelet space
    + Sparse data relatively easy to model
    + <span style="color:red">_Is this related to the fact that all the localised 'differences' at finer granularities mainl show up as 0s due to the spatial structure of the data? Hence spatial similarity creates sparsity in the wavelet space, whereas signal is exposed as the 'non-sparse' area?_</span>
    
### Background
#### DNase-seq and chromatin accessibility
* Enzyme called DNase I selectively cuts DNA at locations where chromatin is accessible
* More accessibility generally $\Rightarrow$ cut more often
* $c_{b}$ = count of reads starting at each base, $b$, in the genome (humans: $b \approx 1,\dots,3 \times 10^9$)
* $d_{b} = \frac{c_{b}}{S}$, where $S$ is total number of mapped reads. This standardises the read counts
* Generally, higher $d_{b} \Rightarrow$ higher accessibility of base $b$ (through DNase I sensitivity, which is a proxy)
* Counts are 0 in many s=places, but where it exists, shows local spatial autocorrelation

#### Wavelets
* Haar Discrete Wavelet Transform (_DWT_)
    + See <span style="color:red"><i>A theory for multiresolution signal decomposition" by S.G. Mallat (1989)</i></span> for a more formal background
* $d = (d_b)^{B}_{b=1}$ are standardised counts from a length $B$ region
* $d$ decomposed into "wavelet coefficients (WCs)", $y_{\textbf{s}cale,\textbf{l}ocation}$, and $y=(y_{sl})$, the vector of all WCs.
    + Zero-cale: $y_{01} = \sum_{b} d_{b}$. Sum of all
    + First scale: $y_{11} = \sum_{b \leq \frac{B}{2}} d_{b} - \sum_{b > \frac{B}{2}} d_{b}$, ie differences between the total counts of the 'first' and 'second' half of the sequence
    + This 'method' of differencing the 'binary split' regions generates finer data at 'higher scale levels/numbers'
* $y = Wd$, and $W$ known as the _DWT matrix_.
    + One-one transform, hence $W$ is invertible, and we get the 'inverse discrete wavelet transform' (IDWT), $d = W^{-1}y$
* Two crucial properties:
    1. 'Whitening property': WCs tend to be less dependent even if $d$ is strongly spatially correlated
    2. Sparsity in 'wavelet denoising': Few big WCs, shrinking/ignoring smaller WCs can provide denoised estimates of signal
    
### Methods
* $N$ individuals
* Local effect expected, hence DNase-seq data divided into regions, with each region tested for association with all near-by Single Nucleotide POlymorphisms (SNPs)
* $d^i$ is a vector of DNase-seq count data for individual $i$ (to $N$). Has length $B=2^J$
* $y^i$ is a vector of WCs from DWT-transformed $d$.
* $g^i \in {0,1,2}$ is $\#$ of copies of minor allele, at a single SNP of interest for inidividual $i$

#### Test setup
* $H_0:$ no association between any WC and $g := \gamma_{sl} = 0 \forall s,l$ (and thus, between $d$ and $g$ also)
* Bayes Factor support for $\gamma_{sl}=1$: (<span style="color:red"> read supplementary material for what is a Bayes Factor! But pretty much a statistic to conduct a hypothesis test in a Bayesian setting.</span>)
$$BF_{sl}(y,g)=\frac{p(y_{sl}|g,\gamma_{sl}=1)}{p(y_{sl}|g,\gamma_{sl}=0)}$$
* Standard normal linear regression for $p(y_{sl}|g,\gamma_{sl})$:
$$y_{sl}^i = \mu_{sl} + \gamma_{sl}\beta_{sl}g^i+\epsilon^i_{sl}$$
$$\epsilon_{sl}^i \sim N(0,\sigma_{sl}^2)$$
* See supplementary material for priors of mu, beta, sigma. Priors were chosen to be able to produce an analytic form solution.
* Hierarchical model: $p(\gamma_{sl}=1|\pi)=\pi_{s}$
    + $\pi_s$ = proportion of WCs at $s$ associated with $g$
    + __Assumption of independence across scales and locations__
    + Likelihood ratio for $\pi = (\pi_1,\dots,\pi_J)$ relative to $\pi \equiv 0$ given on page 7. Full derivation involves joint over all $s,l$ and multiplying out the two states of $\gamma_{sl}$
$$ \Lambda(\pi;y,g) = \prod_{s,l} [\pi_{s}BF_{sl}+(1-\pi_{s})]$$
* Null holds if $\pi \equiv 0$, therefore, likelihood ratio statistic to test $H_0$ is:
$$\hat{\Lambda}(y,g) := \Lambda(\hat{\pi};y,g)$$
* $\hat{\pi}$ is the maximum likelihood estimate of $\pi$, parameterised using an EM algo.
* 'whitening property': $y_{sl}, \beta_{sl}$ conditionally independent given $\pi$ across $s,l$ (moreso than the $d$'s)

#### Multiple SNPs, permutation procedure
* Permutation procedure to assess significance of observed $\hat{\Lambda}$
* Single SNP with vector $g$ vs $d$: $\hat{\Lambda}$ used to test this association
* Association between $d$ and ANY of $P$ nearby SNPs, with $g_1,\dots,g_P$:
$$ \hat{\Lambda}_{max} := \max_{p} \hat{\Lambda}(y,g_p) $$
    + Assess significance by generating permutations $\nu_1,\dots,\nu_M$ of $(1,\dots,M)$
    + Generating random combos of $g^{i}$'s observed from each of the $i\in(1,\dots,M)$ individuals
$$ \hat{\Lambda}_{max}^j := \max_{p} \hat{\Lambda}(y,\nu_{j}(g_p)) $$
$$ p-val_{\hat{\Lambda_{max}}} = \frac{\#\{j:\hat{\Lambda}_{max}^j \geq \hat{\Lambda}_{max}\}+1}{M+1} $$
    + This is effectively testing for significance by 'simulating' the statistic across a heap of other simulated/potential g_p vectors, and assessing the values of the resultant statistic
    
#### Filtering low count WCs
* Just contribute noise (sampling error) and obscure the signal, so filter them out by allocating $BF_{sl}=1$, implying no information about association ($p(y|g)$ same for both $\gamma_{sl}=0,1$)
* WC set $\{y_{sl}^i\}_{i=1}^N$ is "low count" if avg number of reads per individual is less than $L=2$ (for a given scale, location)
    + Tried 1,2,3,5,10. Performance identical for $L \in \{2,3,5\}$, but not for 1,10, suggesting 1 was not filtering enough, 10 was filtering too much (perhaps).
    
#### Quantile transformation to guard against non-normality
* Model assumes normally distributed residuals
* Significant non-normal residuals can arise
* To prevent these from distorting the signal, WCs are quantile-transformed to the quantiles of a standard normal distribution
    + This means they are ranked, and their ranking percentile (percent_rank) is converted to an equivalent value on a standard normal distribution (eg: 1, 10, 100 may become -3, 0, 3)
    + Guarantees that normality of residuals assumption holds under the null, and that Bayes Factors are well behaved under the null
    
#### Controlling for confounding factors
* PCA regression (regress the first principal components on the quantile transformed WCs), and take the residuals.
* Residuals are then quantile transformed and used in the Bayes Factor calculations <span style="color:red">How? Are these used as $y_i$'s?</span>

#### Effect size estimates
* All of this means we can derive posterior distributions on the effect sizes, but only based on data in the wavelet space.
    + Given $\hat{\pi}$, we can derive $p(\beta_{sl}|y,g,\hat{\pi})$
    + Has a point mass at zero, and a 3-parameter t-distribution (see supplementary material)
* To make them meaningful, need to inverse discrete wavelet transform (IDWT) back to interpretable effects
* Data form of the linear regression:
    + Each individual, $i \in {1,\dots,n}$ with wavelet coefficients from locations $l\in{1,\dots,B}$, where $B$ is the length of the genome, allows us to write:
$$ Y = M + \beta g + E$$
* $Y$ is a $B \times N$ matrix (each column is an individual, with a $B \times 1$ vector of observed counts along the genome, transformed into WCs)
* $M$ is a $B \times N$ matrix of means. Same with $E$ for error terms
* $\beta$ is $B \times 1$ - effect for each location
* $g$ is $1 \times N$ - differnt genotypes for each individual
* $D = W^{-1}Y$ from IDWT before. Pre-multiply by $W^{-1}$ to 'convert' our $Y$ into $D$, and we get
$$ W^{-1}Y = W^{-1}(\dots) \\
\therefore D = W^{-1}M + W^{-1}\beta g + W^{-1}E \\
\therefore D = W^{-1}M + \alpha g + W^{-1}E $$
* Because $\alpha$ has linear r/ship with $\beta$, we get a closed form pointwise posterior mean and variance for $\alpha_b$, where $b = 1,\dots,B$ (ie the effect size at each location on the genotype)


